{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9420a5f-678e-4d3c-a9a9-d61705aa8fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import pandas as pd\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24f1e845-8ff2-4b67-8158-847fbe870a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Q1.Build any model of your choice with tunable hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cacb3d49-f8a1-483e-9917-da85ae68c621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"F1_Model\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eab07d3-9be7-4535-9cc7-41480401e838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3://columbia-gr5069-main/raw/results.csv\", header=True, inferSchema=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074f37e0-221d-452a-a7f4-03d67d65babc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.na.drop()\n",
    "df = df.withColumn('rank', when(col('rank').cast('double').isNotNull(), col('rank').cast('double')).otherwise(0)) \\\n",
    "       .withColumn('grid', when(col('grid').cast('double').isNotNull(), col('grid').cast('double')).otherwise(0)) \\\n",
    "       .withColumn('number', when(col('number').cast('double').isNotNull(), col('number').cast('double')).otherwise(0)) \\\n",
    "       .withColumn('points', when(col('points').cast('double').isNotNull(), col('points').cast('double')).otherwise(0)) \\\n",
    "       .withColumn('laps', when(col('laps').cast('double').isNotNull(), col('laps').cast('double')).otherwise(0))\n",
    "assembler = VectorAssembler(inputCols=['grid', 'number', 'points', 'laps'], outputCol='features')\n",
    "data = assembler.transform(df).select('features', 'rank').withColumnRenamed('rank', 'label')\n",
    "train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
    "rf = RandomForestClassifier(labelCol='label', featuresCol='features')\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.numTrees, [5, 10])\n",
    "             .addGrid(rf.maxDepth, [5, 10])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=rf, evaluator=MulticlassClassificationEvaluator(), estimatorParamMaps=paramGrid, numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baa49e75-0d5f-421a-9915-075360025458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"/Workspace/Users/zl3373@columbia.edu/F1_ML_Experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ebeccf-260f-4ee0-a8d3-7f204299c786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Combined Q2 and Q3 together. It may take more than 1 min for one run and approximatly 12 mins in total for 10 runs\n",
    "## Q2.Create an experiment setup where - for each run - you log: \n",
    "- the hyperparameters used in the model\n",
    "- the model itself\n",
    "- every possible metric from the model you chose\n",
    "- at least two artifacts (plots, or csv files)\n",
    "## Q3.Track your MLFlow experiment and run at least 10 experiments with different parameters each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841b04b1-abd9-4619-a168-b328710e0a82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    with mlflow.start_run():\n",
    "        cvModel = cv.fit(train)\n",
    "        bestModel = cvModel.bestModel\n",
    "        # MAE\n",
    "        evaluator_mae = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='mae')\n",
    "        predictions = bestModel.transform(test)\n",
    "        mae = evaluator_mae.evaluate(predictions)\n",
    "        # MSE\n",
    "        evaluator_mse = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='mse')\n",
    "        mse = evaluator_mse.evaluate(predictions)\n",
    "        # R2\n",
    "        evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')\n",
    "        r2 = evaluator_r2.evaluate(predictions)     \n",
    "        # Log hyperparameters and metrics\n",
    "        mlflow.log_params(bestModel.extractParamMap())\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"mse\", mse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.spark.log_model(bestModel, \"random_forest_model\")\n",
    "        # Feature Importance\n",
    "        feature_importances = bestModel.featureImportances.toArray()\n",
    "        feature_names = assembler.getInputCols()\n",
    "        fi = pd.DataFrame(list(zip(feature_names, feature_importances)), columns=[\"Feature\", \"Importance\"])\n",
    "        fi = fi.sort_values(by=\"Importance\", ascending=False)\n",
    "        fi.to_csv(\"/dbfs/tmp/feature_importance.csv\", index=False)\n",
    "        mlflow.log_artifact(\"/dbfs/tmp/feature_importance.csv\", artifact_path=\"feature_importance\")\n",
    "        # Residuals\n",
    "        residuals = predictions.withColumn(\"residual\", col(\"label\") - col(\"prediction\"))\n",
    "        residuals_pd = residuals.select(\"label\", \"prediction\", \"residual\").toPandas()\n",
    "        residuals_pd.to_csv(\"/dbfs/tmp/residuals.csv\", index=False)\n",
    "        mlflow.log_artifact(\"/dbfs/tmp/residuals.csv\", artifact_path=\"residuals\")\n",
    "        # Confusion Matrix \n",
    "        cm = predictions.groupBy('label', 'prediction').count().toPandas()\n",
    "        confusion_matrix = cm.pivot(index='label', columns='prediction', values='count').fillna(0)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(confusion_matrix, annot=True, fmt=\"g\", cmap=\"Blues\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.savefig(\"confusion_matrix.png\")\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "        # CSV\n",
    "        predictions.toPandas().to_csv(\"predictions.csv\", index=False)\n",
    "        mlflow.log_artifact(\"predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aefbe6a9-b994-40ee-820f-1325d95de7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Q4.Select your best model run and explain why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1831d5b-2a9b-4893-9ee8-4e99fbce4553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_best_model():\n",
    "    client = MlflowClient()\n",
    "    experiment_id = \"1571216335170020\"  \n",
    "    runs = client.search_runs(experiment_id)\n",
    "    metrics_data = []\n",
    "    for run in runs:\n",
    "        run_id = run.info.run_id\n",
    "        metrics = run.data.metrics\n",
    "        mae = metrics.get(\"mae\")\n",
    "        mse = metrics.get(\"mse\")\n",
    "        r2 = metrics.get(\"r2\")\n",
    "        metrics_data.append({\n",
    "            \"run_id\": run_id,\n",
    "            \"mae\": mae,\n",
    "            \"mse\": mse,\n",
    "            \"r2\": r2\n",
    "        })\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    # Select the best model with lowest MAE and lowest MSE, and highest R²\n",
    "    best_run = df.loc[(df['mae'].idxmin()) & (df['mse'].idxmin()) & (df['r2'].idxmax())]\n",
    "    print(\"Best Model Run ID:\", best_run[\"run_id\"])\n",
    "    print(\"MAE:\", best_run[\"mae\"])\n",
    "    print(\"MSE:\", best_run[\"mse\"])\n",
    "    print(\"R²:\", best_run[\"r2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72286d38-e1d9-4f54-bfdc-d1cce3c7cbcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_best_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "030b8647-84b1-432f-894e-0c08278814da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " I finnally decide to choose the model with Run ID: 6bf61e1172a84db6bb11a38cb11dadfa. I utilized the methods to select one with relatively lower Mean Absolute Error (MAE), lower Mean Squared Error (MSE), and higher R-squared (R²) value. The model with Run ID 6bf61e1172a84db6bb11a38cb11dadfa achieved an MAE of approximately 2.87, indicating minimal deviation between predicted and actual values, and an MSE of around 39.97, reflecting a relatively small squared error."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HW3 coding",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
